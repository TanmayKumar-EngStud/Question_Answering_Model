{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('DeepLearning': conda)"
    },
    "interpreter": {
      "hash": "ce5412b8715117908d8129964960c3be480f008a4994ece1be9c3b7cf4dabcdb"
    },
    "colab": {
      "name": "New_word2vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanmayKumar-EngStud/Question_Answering_Model/blob/main/New_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IF-Sg7Pd10N"
      },
      "source": [
        "# **Mounting the Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTXwwT_hY36Q",
        "outputId": "4046d75a-0978-4f5b-b879-8f47e04eff18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XbmGroROOPM"
      },
      "source": [
        "import re , requests\n",
        "#First: We are just scrape the data from the website\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xr5tkjUOOPY"
      },
      "source": [
        "BaseSite ='https://en.wikipedia.org/wiki/Outline_of_health'\n",
        "data = requests.get(BaseSite)\n",
        "\n",
        "LinksList= re.findall('(?<=<a href=\")/wiki/[A-Za-z ]+(?=\")',data.text)\n",
        "\n",
        "unique = set(LinksList)\n",
        "LinksList = list(unique)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG5hs-w3fL4Q"
      },
      "source": [
        "## **This function is for Refining the overall Text**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urdhr_w-OOPZ"
      },
      "source": [
        "def refine(ListText):\n",
        "    inside = False\n",
        "    newText = ''\n",
        "    for text in ListText:\n",
        "        for word in text:\n",
        "            if word == '<':\n",
        "                inside =True\n",
        "            elif word == '>':\n",
        "                inside = False\n",
        "                continue\n",
        "            if not inside:\n",
        "                newText = newText + word\n",
        "                #This must eleminate all the attributes\n",
        "    newText1 = ''\n",
        "    for letter in newText:\n",
        "        if letter == '[' or letter == '(':\n",
        "            inside = True\n",
        "        elif letter == ']' or letter == ')':\n",
        "            inside = False\n",
        "            continue\n",
        "        if not inside:\n",
        "            newText1 = newText1 + letter\n",
        "\n",
        "    return newText1\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huAVEUL5OOPd"
      },
      "source": [
        "def clean(text):\n",
        "    temp = re.findall(\"(?<=<p>).*(?=</)\",str(text))\n",
        "    return temp"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1izj4SZFfVmi"
      },
      "source": [
        "## Making the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiLFsah5YMfo"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class callback(CallbackAny2Vec):\n",
        "    def __init__(self):\n",
        "        self.epoch = 1\n",
        "    def on_epoch_end(self,model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "\n",
        "        if self.epoch == 0:\n",
        "            print(f\"Loss after epoch {self.epoch}: {loss}\")\n",
        "        elif self.epoch%100 == 0:\n",
        "            print(f\"Loss after epoch {self.epoch}: {loss - self.loss_previous_step}\")\n",
        "        self.epoch+=1\n",
        "        self.loss_previous_step = loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEC5sS56fcqW"
      },
      "source": [
        "#### **Function for making the list of sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRAotesTdVwK"
      },
      "source": [
        "def makeSentence(Paragraph):\n",
        "    Sentences = []\n",
        "    for sentence in Paragraph.split('. '):\n",
        "        Sentences.append(sentence)\n",
        "    return Sentences"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKGaHY5ffnTp"
      },
      "source": [
        "### ***For the generation of the first Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYFh10oDYtZF"
      },
      "source": [
        "#This is the one time code \n",
        "def buildFirst_Model(text):\n",
        "    w2v = Word2Vec(size = 100,\n",
        "                sg = 1,\n",
        "                min_count = 1,  \n",
        "                window = 5,\n",
        "                workers = 3)\n",
        "    #initializing the word2vec for the first time\n",
        "\n",
        "\n",
        "\n",
        "    #Now building up the vocabulary\n",
        "    sentences = makeSentence(text)\n",
        "    w2v.build_vocab(sentences)\n",
        "\n",
        "    w2v.train(sentences,\n",
        "                total_examples=w2v.corpus_count,\n",
        "                epochs =1001,\n",
        "                report_delay=1,\n",
        "                compute_loss=True,\n",
        "                callbacks=[callback()])\n",
        "    w2v.save(f'/content/drive/MyDrive/NLP Work flow/Trained_Models/w2vModel_Version{1}.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQp1QO6hXeQ"
      },
      "source": [
        "### **When the first model is generated then we have to load it and simplimg retrain it.**\n",
        "<hr>\n",
        "\n",
        "**Key Features:** <br>\n",
        " \n",
        "\n",
        "*   This will modify the alrady generated Word2Vector model.\n",
        "*   This are versatile method and will be really easy for retraining and refining the model in a dynamic way.\n",
        "*   Every modification of the model saves the new version of that model.\n",
        "*   Every iteration brings the latest model for retraining.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxGJPTefj8t9"
      },
      "source": [
        "import os\n",
        "def findLatestModel(path):\n",
        "    ListofFiles = os.listdir(path)\n",
        "    if ListofFiles == None:\n",
        "        return None\n",
        "    return ListofFiles[-1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YLgHGF8jKeq"
      },
      "source": [
        "def Modify_Model(text):\n",
        "    mainPath = '/content/drive/MyDrive/NLP Work flow/Trained_Models/'\n",
        "    fileName = findLatestModel(mainPath)\n",
        "    Lw2v = Word2Vec.load(mainPath+fileName)\n",
        "    num = int(re.findall('(?<=Model_Version)[0-9]+(?=.h5)', fileName)[0])\n",
        "    sentence = makeSentence(text)\n",
        "    print(f'Doing Model traing; Iteration: {num +1}')\n",
        "    Lw2v.build_vocab(sentence, update = True)\n",
        "    token_count = sum([len(se) for se in sentence])\n",
        "\n",
        "    print(f'\\n\\nTHIS IS TOKEN COUNT!! : {token_count}')\n",
        "    Lw2v.train(sentence,\n",
        "                epochs =1001,\n",
        "                total_words = token_count,\n",
        "                report_delay=1,\n",
        "                compute_loss=True,\n",
        "                callbacks=[callback()])\n",
        "    print(f'Done Model training; Iteration:{num+1}')\n",
        "    Lw2v.train(f'{mainPath}w2vModel_Version{num+1}.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "tDCgOkVuOOPg",
        "outputId": "cae153c2-fb7e-4d6e-9277-4d7765634d6a"
      },
      "source": [
        "initial = True\n",
        "for RawLink in LinksList:\n",
        "    \n",
        "    link= \"https://en.wikipedia.org/\"+RawLink\n",
        "    temp = requests.get(link)\n",
        "    soup = BeautifulSoup(temp.content, \"html.parser\")\n",
        "    text= clean(soup)\n",
        "    text= refine(text)\n",
        "    if initial :\n",
        "        buildFirst_Model(text)\n",
        "        initial = False\n",
        "    else :\n",
        "        Modify_Model(text)\n",
        "   \n",
        "#Achieved all the rough data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after epoch 100: 3780.65625\n",
            "Loss after epoch 200: 3946.125\n",
            "Loss after epoch 300: 3806.375\n",
            "Loss after epoch 400: 3570.75\n",
            "Loss after epoch 500: 3656.625\n",
            "Loss after epoch 600: 3889.25\n",
            "Loss after epoch 700: 3930.5\n",
            "Loss after epoch 800: 4224.0\n",
            "Loss after epoch 900: 3537.0\n",
            "Loss after epoch 1000: 4386.75\n",
            "Doing Model traing; Iteration: 2\n",
            "\n",
            "\n",
            "THIS IS TOKEN COUNT!! : 22276\n",
            "Loss after epoch 100: 28881.75\n",
            "Loss after epoch 200: 23429.5\n",
            "Loss after epoch 300: 22598.0\n",
            "Loss after epoch 400: 18917.0\n",
            "Loss after epoch 500: 19468.0\n",
            "Loss after epoch 600: 19062.0\n",
            "Loss after epoch 700: 19172.0\n",
            "Loss after epoch 800: 20460.0\n",
            "Loss after epoch 900: 21164.0\n",
            "Loss after epoch 1000: 20574.0\n",
            "Done Model training; Iteration:2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2ea0ad47d8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mModify_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Achieved all the rough data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-bc7658a190a4>\u001b[0m in \u001b[0;36mModify_Model\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 callbacks=[callback()])\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Done Model training; Iteration:{num+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mLw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{mainPath}w2vModel_Version{num+1}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1200\u001b[0;31m                 \u001b[0;34m\"You must specify either total_examples or total_words, for proper job parameters updation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m                 \u001b[0;34m\"and progress calculations. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;34m\"The usual value is total_examples=model.corpus_count.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count."
          ]
        }
      ]
    }
  ]
}